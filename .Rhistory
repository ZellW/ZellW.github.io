V(g)$label.color <- rgb(0, 0, .2, .8)
V(g)$frame.color <- NA
egam <- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4)
E(g)$color <- rgb(.5, .5, 0, egam)
E(g)$width <- egam
# plot the graph in layout1
plot(g, layout=layout1)
tkplot(g, layout=layout.kamada.kawai)
plot(g, layout=layout.kamada.kawai)
library(mrsdeploy)
# Create glm model with `mtcars` dataset
carsModel <- glm(formula = am ~ hp + wt, data = mtcars, family = binomial)
# Produce a prediction function that can use the model
manualTransmission <- function(hp, wt) {
newdata <- data.frame(hp = hp, wt = wt)
predict(carsModel, newdata, type = "response")
}
# test function locally by printing results
print(manualTransmission(120, 2.8)) # 0.6418125
remoteLogin("http://awsdbedwqa01:12800", username = "admin", password = "AdminPW#1",diff = TRUE,session = FALSE,commandline = TRUE)
api <- publishService(
"mtService", code = manualTransmission, model = carsModel, inputs = list(hp = "numeric", wt = "numeric"),
outputs = list(answer = "numeric"), v = "v1.0.0")
# Print capabilities that define the service holdings: service
# name, version, descriptions, inputs, outputs, and the
# name of the function to be consumed
print(api$capabilities())
# Consume service by calling function, `manualTransmission`
# contained in this service
result <- api$manualTransmission(120, 2.8)
# Print response output named `answer`
print(result$output("answer")) # 0.6418125
IsJSON <- FALSE# FALSE - CSV; TRUE - JSON
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("httr", "rlist", "svDialogs", "ggplot2", "jsonlite", "gridExtra", "readr", "dplyr", prompt = FALSE)
timeStamp <-  strftime(Sys.time(),"%Y%m%d%H")
newData <- file.choose()
if(!IsJSON){
JSONtoCSV_LS <- read_csv(newData, col_names =TRUE)#Used when input file is a CSV
}else{
JSONdata_LS <- read_csv(newData, col_names = TRUE)#Used when input file is JSON
#Skip look is file is already CSV
for(x in 1:nrow(JSONdata_LS)){
tmpJSON <- as.character(JSONdata_LS[x,2])
#Convert to data frame JSONlite
tmpJSON <- fromJSON(tmpJSON)
if(x==1){JSONtoCSV_LS <- tmpJSON}
else{JSONtoCSV_LS <- rbind(JSONtoCSV_LS, tmpJSON)}
}
tmpJSON <- NULL
x <-NULL
#Make sure the target variable is not in the sample file.  Example:
#JSONtoCSV_LS <- select(newData, -Funded)
}
CSV_file <- paste("./data/", "LeadScoreCSV_", timeStamp, ".csv", sep="")
if(nrow(JSONtoCSV_LS) < 1000){nrowCnt = nrow(JSONtoCSV_LS)} else{nrowCnt=1000}
write.csv(JSONtoCSV_LS[1:nrowCnt,], CSV_file, row.names = FALSE)
#LS_Primary_LF_0315 (created 3/16/17)  New model created 3/22/17 LS_LFSF_AllExchanges_0321; New model created 5/9/17 LS_LFSF_AllExchanges_0504
#Model Name: AVG Blender M39 40 38 M46
# PROJECT ID AND MODEL ID
PID = "590a7cfdc808910e34816665"
MID = "590a9dc2c64f6c15657a7f36"
# API ENDPOINT AND KEY - PRODUCTION
# This is dedicated instance only for prediction.
# No concurrency with other cloud users.
# It's only accessible with datarobot-key through v1 api for LendingTree.
API = 'https://lendingtree.orm.datarobot.com/api/v1/'
DR_key = 'aa093a7a-370d-38b4-d7f9-112f37ea20eb'
# API ENDPOINT AND KEY - BUILDING
# This is for shared cloud resources. May be concurrency for resources with
# everybody else.  Same options that you can get from Datarobot web interface
# (modeling, leaderboard, feature impact etc).  Accessible through v2 api.
# API = 'https://app.datarobot.com/api/v2'
# DR_key = ''  #Not Needed
# USERNAME = EMAIL ADDRESS
user = 'apiuser_datarobot@lendingtree.com'
# API TOKEN - FIND IT IN YOUR GUI PROFILE
token = 'vZtV22rL-7salcFfxCoVOgP4azWT0o6M'
# DATASET FILEPATH
dataset = CSV_file
predictions <- fromJSON(content(POST(paste0(API, PID, '/', MID, '/predict'),
c(authenticate(user, token)),
add_headers("datarobot-key" = DR_key),type="text/plain; encoding=UTF-8",
body=upload_file(dataset)),"text"))
flattenJSONdata <- list.flatten(predictions)
predictionsDF <- as.data.frame(flattenJSONdata[7]$predictions.class_probabilities.1.0)
colnames(predictionsDF) <- c("LeadScore")
View(predictionsDF)
View(predictionsDF)
write.csv(predictionsDF, "tmpPredictions.csv")
getwd()
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("class", "gmodels", "dplyr", "tm", "e1071", prompt = FALSE)#kernlab needed for spam data
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
glimpse(sms_raw)
names(sms_raw)<-c("type","text")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
table(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
length(sms_corpus)
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm <- DocumentTermMatrix(corpus_clean)
install.packages("SnowballC")
library(SnowballC)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")
table(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(corpus_clean)
?iconv
sms_raw_train <- sms_raw[1:2388, ]
sms_raw_test  <- sms_raw[2389:3184, ]
sms_dtm_train <- sms_dtm[1:2388, ]
sms_dtm_test  <- sms_dtm[2389:3184, ]
sms_corpus_train <- corpus_clean[1:2388]
sms_corpus_test  <- corpus_clean[2389:3184]
Dictionary <- function(x) {
if( is.character(x) ) {
return (x)
}
stop('x is not a character vector')
}
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test  <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
return(x)
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_test, MARGIN = 2, convert_counts)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
CrossTable(sms_test_pred, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
sms_test_pred <- predict(sms_classifier, sms_test)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_corpus <- Corpus(VectorSource(sms_raw$text))
length(sms_corpus)
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_dtm <- DocumentTermMatrix(corpus_clean)
library(SnowballC)
sms_dtm <- DocumentTermMatrix(corpus_clean)
DocumentTermMatrix
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
View(sms_raw)
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_raw$text <- iconv(sms_raw$text, to = "utf-8", sub="")#VERY HELPFUL
sms_raw$text <- iconv(sms_raw$text, to = "utf-8")#VERY HELPFUL
sms_corpus <- Corpus(VectorSource(sms_raw$text), encoding="UTF-8")
sms_corpus <- Corpus(VectorSource(sms_raw$text), encoding="UTF-8")
sms_corpus <- Corpus(VectorSource(sms_raw$text),readerControl = list(language="en_US"))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text),readerControl = list(language="en_US"))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw <- read.delim("../ML/data/sms_raw.txt", header=FALSE)
names(sms_raw)<-c("type","text")
sms_raw$type <- factor(sms_raw$type)
sms_raw$text <- iconv(sms_raw$text, to = "utf-8")#VERY HELPFUL
sms_corpus <- Corpus(VectorSource(sms_raw$text),readerControl = list(language="en_US"))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw_train <- sms_raw[1:2388, ]
sms_raw_test  <- sms_raw[2389:3184, ]
length(sms_dtm)
count(sms_dtm)
nrow(sms)dtm
nrow(smsdtm)
sms_dtm_train <- sms_dtm[1:2388, ]
sms_dtm_test  <- sms_dtm[2389:3184, ]
sms_corpus_train <- corpus_clean[1:2388]
sms_corpus_test  <- corpus_clean[2389:3184]
Dictionary <- function(x) {
if( is.character(x) ) {
return (x)
}
stop('x is not a character vector')
}
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test  <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
return(x)
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_test, MARGIN = 2, convert_counts)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
CrossTable(sms_test_pred, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("MASS", prompt = FALSE)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("MASS", "dplyr", prompt = FALSE)
glimpse(fgl)
data.lda <- lda(formula = type ~ ., data = fgl)
data.lda
data.lda.pred <-predict(data.lda, newdata=fgl[,c(1:9)])$class
data.lda.pred
table(data.lda.pred,fgl[,10])
data_set <- data.frame(
id=c(1,2,3,4,5),
mood=c(0,20,20,40,50),
value=c(12.34, 32.2, 24.3, 83.1, 8.32),
outcome=c(1,1,0,0,0))
feature_name = 'mood'
data_set[,feature_name][!is.na(data_set[,feature_name])]
data_set[,feature_name][!is.na(data_set[,feature_name])]
data_set[,feature_name][!is.na(data_set[,feature_name])]
library(dplyr)
distinct(data_set, mood)
data_set[,feature_name][!is.na(data_set[,feature_name])]
data_set %>% filter(!is.na(mood))%>% distinct(data_set, mood)
data_set %>% filter(!is.na(mood)) %>% distinct(data_set, mood)
data_set %>% filter(!is.na(mood)) %>% distinct(mood)
count(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
length(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood))
length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2
nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood)) > 2
Get_Free_Text_Measures <- function(data_set, minimum_unique_threshold=0.9, features_to_ignore=c()){
text_features <- c(names(data_set[sapply(data_set, is.character)]), names(data_set[sapply(data_set, is.factor)]))
for (f_name in setdiff(text_features, features_to_ignore)){
f_vector <- as.character(data_set[,f_name])
if (length(unique(as.character(f_vector))) > (nrow(data_set) * minimum_unique_threshold)){
data_set[,paste0(f_name, '_word_count')] <- sapply(strsplit(f_vector, " "), length)
data_set[,paste0(f_name, '_character_count')] <- nchar(as.character(f_vector))
data_set[,paste0(f_name, '_first_word')] <- sapply(strsplit(as.character(f_vector), " "), `[`, 1)
data_set[,f_name] <- NULL
}
}
return(data_set)
}
Titanic_dataset_temp <- Get_Free_Text_Measures(data_set = Titanic_dataset_temp, features_to_ignore = c())
Titanic_dataset_temp <- Titanic_dataset#keep the original virgin data set
Titanic_dataset_temp <- Titanic_dataset
# load the data set in case you haven't already done so
Titanic_dataset <- read.table('http://math.ucdenver.edu/RTutorial/titanic.txt', sep='\t', header=TRUE, stringsAsFactors = FALSE)
head(Titanic_dataset)
Titanic_dataset_temp <- Titanic_dataset
#Calculate the number of words in the Name field - just uses spaces - I think this is unreliable - OK for just names
#Keeps the comma.  My wordcount function below does not (of course we are just counting so this is OK)
#sapply simple gives you the number of words left by strsplit
Titanic_dataset_temp$Word_Count <- sapply(strsplit(Titanic_dataset_temp$Name, " "), length)
print(head(Titanic_dataset_temp))
Titanic_dataset_temp <- Get_Free_Text_Measures(data_set = Titanic_dataset_temp, features_to_ignore = c())
str(Titanic_dataset_temp)
if(!require(easypackages)){
install.packages("easypackages")
library(easypackages)
}
packages("readr", "lubridate", "dplyr", "ggplot2", "infotheo", prompt = FALSE)
Feature_Engineer_Dates <- function(data_set, remove_original_date=TRUE) {
data_set <- data.frame(data_set)
date_features <- names(data_set[sapply(data_set, is.Date)])#is.Date is from lubridate
for (feature_name in date_features) {
data_set[,paste0(feature_name,'_DateInt')] <- as.numeric(data_set[,feature_name])
data_set[,paste0(feature_name,'_Month')] <- as.integer(format(data_set[, feature_name], "%m"))
data_set[,paste0(feature_name,'_ShortYear')] <- as.integer(format(data_set[,feature_name], "%y"))
data_set[,paste0(feature_name,'_LongYear')] <- as.integer(format(data_set[,feature_name], "%Y"))
data_set[,paste0(feature_name,'_Day')] <- as.integer(format(data_set[,feature_name], "%d"))
# week day number requires first pulling the weekday label,
# creating the 7 week day levels, and casting to integer
data_set[,paste0(feature_name,'_WeekDayNumber')] <- as.factor(weekdays(data_set[, feature_name]))
levels(data_set[, paste0(feature_name, '_WeekDayNumber')]) <-
list(Monday=1, Tuesday=2, Wednesday=3, Thursday=4, Friday=5, Saturday=6, Sunday=7)
data_set[, paste0(feature_name,'_WeekDayNumber')] <-
as.integer(data_set[,paste0(feature_name,'_WeekDayNumber')])
data_set[,paste0(feature_name,'_IsWeekend')] <-
as.numeric(grepl("Saturday|Sunday", weekdays(data_set[, feature_name])))
data_set[,paste0(feature_name,'_YearDayCount')] <- yday(data_set[,feature_name])
data_set[,paste0(feature_name,'_Quarter')] <- lubridate::quarter(data_set[, feature_name], with_year = FALSE)
data_set[,paste0(feature_name,'_Quarter')] <- lubridate::quarter(data_set[, feature_name], with_year = TRUE)
if (remove_original_date)#delete the original date column only if remove_original_date=TRUE in function call
data_set[, feature_name] <- NULL
}
return(data_set)
}
mix_dataset <- data.frame(id=c(10,20,30,40,50), gender=c('male','female','female','male','female'),
some_date=c('2012-01-12','2012-01-12','2012-12-01','2012-05-30','2013-12-12'),
value=c(12.34, 32.2, 24.3, 83.1, 8.32), outcome=c(1,1,0,0,0))
write_csv(mix_dataset, './data/mix_dataset.csv')
mix_dataset <- read_csv('./data/mix_dataset.csv')
mix_dataset <- Feature_Engineer_Dates(mix_dataset)
head(mix_dataset)
Feature_Engineer_Integers <- function(data_set, features_to_ignore=c()) {
data_set <- data.frame(data_set)
for (feature_name in setdiff(names(data_set), features_to_ignore)) {
if (class(data_set[,feature_name])=='numeric' | class(data_set[,feature_name])=='integer') {
feature_vector <- data_set[,feature_name]
if (all((feature_vector - round(feature_vector)) == 0)) {
# make sure we have more than 2 values excluding NAs
if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2) {
#print(feature_name)#not really needed/helpful
data_set[,paste0(feature_name,'_IsZero')] <- ifelse(data_set[,feature_name]==0,1,0)
data_set[,paste0(feature_name,'_IsPositive')] <- ifelse(data_set[,feature_name]>=0,1,0)
# separate data into two bins using infotheo
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=2)
data_set[,paste0(feature_name,'_2Bins')] <- data_discretized$X
if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 4) {
# try 4 bins
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=4)
data_set[,paste0(feature_name,'_4Bins')] <- data_discretized$X
}
}
}
}
}
return (data_set)
}
Feature_Engineer_Integers <- function(data_set, features_to_ignore=c()) {
data_set <- data.frame(data_set)
for (feature_name in setdiff(names(data_set), features_to_ignore)) {
if (class(data_set[,feature_name])=='numeric' | class(data_set[,feature_name])=='integer') {
feature_vector <- data_set[,feature_name]
if (all((feature_vector - round(feature_vector)) == 0)) {
# make sure we have more than 2 values excluding NAs
# if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2) {
if (nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood)) > 2) {
#print(feature_name)#not really needed/helpful
data_set[,paste0(feature_name,'_IsZero')] <- ifelse(data_set[,feature_name]==0,1,0)
data_set[,paste0(feature_name,'_IsPositive')] <- ifelse(data_set[,feature_name]>=0,1,0)
# separate data into two bins using infotheo
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=2)
data_set[,paste0(feature_name,'_2Bins')] <- data_discretized$X
if (nrow(data_set %>% filter(!is.na(mood)) %>% distinct(mood)) > 4) {
# try 4 bins
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=4)
data_set[,paste0(feature_name,'_4Bins')] <- data_discretized$X
}
}
}
}
}
return (data_set)
}
mix_dataset <- read_csv('./data/mix_dataset.csv')
Feature_Engineer_Integers(mix_dataset, features_to_ignore=c('id'))
Feature_Engineer_Integers <- function(data_set, features_to_ignore=c()) {
data_set <- data.frame(data_set)
for (feature_name in setdiff(names(data_set), features_to_ignore)) {
if (class(data_set[,feature_name])=='numeric' | class(data_set[,feature_name])=='integer') {
feature_vector <- data_set[,feature_name]
if (all((feature_vector - round(feature_vector)) == 0)) {
# make sure we have more than 2 values excluding NAs
# if (length(unique(data_set[,feature_name][!is.na(data_set[,feature_name])])) > 2) {
if (nrow(data_set %>% filter(!is.na(feature_vector)) %>% distinct(feature_vector)) > 2) {
#print(feature_name)#not really needed/helpful
data_set[,paste0(feature_name,'_IsZero')] <- ifelse(data_set[,feature_name]==0,1,0)
data_set[,paste0(feature_name,'_IsPositive')] <- ifelse(data_set[,feature_name]>=0,1,0)
# separate data into two bins using infotheo
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=2)
data_set[,paste0(feature_name,'_2Bins')] <- data_discretized$X
if (nrow(data_set %>% filter(!is.na(feature_vector)) %>% distinct(feature_vector)) > 4) {
# try 4 bins
data_discretized <- discretize(data_set[,feature_name], disc='equalfreq', nbins=4)
data_set[,paste0(feature_name,'_4Bins')] <- data_discretized$X
}
}
}
}
}
return (data_set)
}
mix_dataset <- read_csv('./data/mix_dataset.csv')
Feature_Engineer_Integers(mix_dataset, features_to_ignore=c('id'))
library(devtools)
devtools::install_github("statswithr/statsr")
library(readr)
tmpDF <- read_csv(file.choose())
View(tmpDF)
View(tmpDF)
tmpDF <- read.csv(file.choose())
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
tmpDF <- tmpDF[-c(42537:42538),]
write.csv(file.choose())
write.csv(tmpDF,"C:\Users\cweaver\Desktop\LendingClubData\LoanStats3a.csv")
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3a.csv")
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
tmpDF <- tmpDF[-c(188182:188183),]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3b.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tail(tmpDF)
View(tmpDF)
tmpDF <- tmpDF[-c(235630:235631),]
View(tmpDF)
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3c.csv")
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
tmpDF <- tmpDF[-c(421096:421097),]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats3d.csv")
tmpDF <- read.csv(file.choose(), skip =1)
View(tmpDF)
nrow(tmpDF)
nrow(tmpDF)-1
tmpDF <- tmpDF[-c(nrow(tmpDF)-1:nrow(tmpDF))]
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c(nrow(tmpDF)-1:nrow(tmpDF)), ]
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q1.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q2.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q3.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2016Q4.csv")
tmpDF <- read.csv(file.choose(), skip =1)
tmpDF <- tmpDF[-c((nrow(tmpDF)-1):nrow(tmpDF)), ]
write.csv(tmpDF,"C:\\Users\\cweaver\\Desktop\\LendingClubData\\LoanStats_2017Q1.csv")
install.packages("plotly")
myData = read_html("https://en.wikipedia.org/wiki/List_of_U.S._states_by_life_expectancy")
## LOAD THE PACKAGES ####
library(rvest)
library(ggplot2)
library(dplyr)
library(scales)
myData = read_html("https://en.wikipedia.org/wiki/List_of_U.S._states_by_life_expectancy")
myData = myData %>% html_nodes("table") %>% .[[2]] %>% html_table(fill=T)
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
library(h2o)
localH2O = h2o.init(max_mem_size = "3G", nthreads = 2)
data("Seatbelts")
summary(Seatbelts)
dim(Seatbelts)
trainHex <- as.h2o(Seatbelts)
x_names  <- colnames(trainHex[2:8])
myModel <- h2o.deeplearning(x = x_names, y = "DriversKilled", training_frame = trainHex)
myModel
h2o.shutdown(prompt = FALSE)
source('~/GitHub/LT/MyLT1_Nikita.R', echo=TRUE)
install.packages("sparklyr")
library(sparklyr)
knitr::include_graphics("./images/eigenvalues.png")
getwd()
setwd("~/GitHub/ZellW.github.io")
knitr::include_graphics("./images/eigenvalues.png")
knitr::include_graphics("../images/eigenvalues.png")
